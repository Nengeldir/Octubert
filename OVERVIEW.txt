╔════════════════════════════════════════════════════════════════════════════╗
║  USER COMMAND                                                              ║
║  python -m smdiff.cli.train --model octuple_ddpm --dataset_id pop909_melody║
╚════════════════════════════════════════════════════════════════════════════╝
                                    ↓
┌────────────────────────────────────────────────────────────────────────────┐
│  1. CLI INITIALIZATION (src/smdiff/cli/train.py:main())                    │
├────────────────────────────────────────────────────────────────────────────┤
│  ┌─ Parse CLI arguments (--model, --dataset_id, --config, --set, etc)     │
│  ├─ load_config(model_id, config_path, set_overrides)                     │
│  │   └─ Merges: base.yaml → models.yaml[model_id] → user.yaml → --set    │
│  ├─ apply_dataset_to_config(cfg, dataset_id)                              │
│  │   └─ Updates cfg with: dataset_path, tracks, tokenizer_id              │
│  ├─ Validate registries:                                                   │
│  │   ├─ resolve_model_id(model) → MODEL_REGISTRY lookup                   │
│  │   ├─ resolve_tokenizer_id(tokenizer_id) → TOKENIZER_REGISTRY lookup    │
│  │   └─ resolve_masking_id(masking_strategy) if present                   │
│  ├─ build_underlying_argv() → converts merged config to legacy hparams    │
│  ├─ get_sampler_hparams('train') → creates hparams object H              │
│  ├─ Enrich H: H.tokenizer_id, H.dataset_id                                │
│  ├─ Set H.log_dir = "runs/{model_id}"                                     │
│  ├─ config_log(H.log_dir) → setup logging to runs/{model_id}/logs/        │
│  ├─ Snapshot configs → runs/{model_id}/configs/ {config.yaml, hparams.yaml}
│  └─ start_training_log(H) → log all hparams                               │
└────────────────────────────────────────────────────────────────────────────┘
                                    ↓
┌────────────────────────────────────────────────────────────────────────────┐
│  2. TRAINER INITIALIZATION (src/smdiff/trainer.py:main(H))               │
├────────────────────────────────────────────────────────────────────────────┤
│  ┌─ Load dataset based on H.dataset_path:                                  │
│  │   ├─ IF os.path.isdir(H.dataset_path):                                  │
│  │   │   └─ OctupleDataset(H.dataset_path, H.NOTES) [per-file loading]   │
│  │   └─ ELSE:                                                              │
│  │       └─ SubseqSampler(np.load(H.dataset_path), H.NOTES) [.npy]       │
│  ├─ Split train/val:                                                       │
│  │   ├─ val_idx = int(len(midi_data) * H.validation_set_size)             │
│  │   ├─ train_loader = DataLoader(train_subset, batch_size, shuffle=True) │
│  │   └─ val_loader = DataLoader(val_subset, batch_size)                   │
│  ├─ Initialize model:                                                      │
│  │   └─ sampler = get_sampler(H).cuda()  [sampler_utils]                  │
│  │       └─ Reads H.model → loads from models/ package                    │
│  ├─ Setup optimizer:                                                       │
│  │   └─ optim = torch.optim.Adam(sampler.parameters(), lr=H.lr)           │
│  ├─ If H.ema:                                                              │
│  │   ├─ ema = EMA(H.ema_beta)  [train_utils]                              │
│  │   └─ ema_sampler = copy.deepcopy(sampler)                              │
│  └─ Initialize training state: losses, val_losses, mean_losses = []       │
└────────────────────────────────────────────────────────────────────────────┘
                                    ↓
┌────────────────────────────────────────────────────────────────────────────┐
│  3. CHECKPOINT RESUME (if H.load_step > 0)                               │
├────────────────────────────────────────────────────────────────────────────┤
│  ┌─ load_model(sampler, H.sampler, H.load_step, H.load_dir) [log_utils]  │
│  ├─ load_model(ema_sampler, f'{H.sampler}_ema', ...) if H.ema              │
│  ├─ load_model(optim, f'{H.sampler}_optim', ...)                          │
│  └─ load_stats(H, H.load_step) [log_utils]                                │
│      └─ Resumes: losses, val_losses, mean_losses arrays                   │
└────────────────────────────────────────────────────────────────────────────┘
                                    ↓
┌────────────────────────────────────────────────────────────────────────────┐
│  4. MAIN TRAINING LOOP (for step in range(start_step, H.train_steps))    │
├────────────────────────────────────────────────────────────────────────────┤
│  ┌─ FOR EACH STEP:                                                         │
│  │  ┌─ sampler.train() / ema_sampler.train()                              │
│  │  ├─ Warmup LR: optim_warmup(H, step, optim) if step <= H.warmup_iters  │
│  │  ├─ Get batch:                                                          │
│  │  │   x = next(train_iterator) → cycle(train_loader)                    │
│  │  │   x = augment_note_tensor(H, x) [train_utils]  [optional augment]   │
│  │  │   x = x.cuda(non_blocking=True)                                     │
│  │  ├─ Forward pass:                                                       │
│  │  │   IF H.amp (Automatic Mixed Precision):                             │
│  │  │   │  └─ with torch.cuda.amp.autocast():                            │
│  │  │   │     stats = sampler.train_iter(x)  [model forward]             │
│  │  │   ELSE:                                                              │
│  │  │      └─ stats = sampler.train_iter(x)                               │
│  │  ├─ Backward pass & optimizer step:                                    │
│  │  │   optim.zero_grad()                                                 │
│  │  │   stats['loss'].backward()                                          │
│  │  │   optim.step() [or scaler.step() if AMP]                           │
│  │  ├─ Append loss: losses = np.append(losses, stats['loss'].item())      │
│  │  └─ sampler.eval() / ema_sampler.eval()                                │
│  │                                                                          │
│  │  ┌─ PERIODIC: step % H.steps_per_log == 0                              │
│  │  │   ├─ mean_loss = np.mean(losses)                                    │
│  │  │   ├─ stats['mean_loss'] = mean_loss                                 │
│  │  │   ├─ log_stats(step, stats) [log_utils]  → print to console/file    │
│  │  │   └─ losses = np.array([])  [reset]                                 │
│  │  │                                                                      │
│  │  ┌─ PERIODIC: step % H.steps_per_update_ema == 0                       │
│  │  │   └─ ema.update_model_average(ema_sampler, sampler) [train_utils]   │
│  │  │                                                                      │
│  │  ┌─ PERIODIC: step % H.steps_per_sample == 0 & step > 0                │
│  │  │   ├─ samples = get_samples(ema_sampler, H.sample_steps, b=...) [sampler_utils]
│  │  │   ├─ save_samples(samples, step, H.log_dir) [log_utils]             │
│  │  │   │   └─ Save to: runs/{model_id}/samples/samples_{step}.npz.npy   │
│  │  │   │   └─ sync_to_home() [cluster/utils.py] → copy to permanent home│
│  │  │   └─ samples_2_noteseq(samples, tokenizer_id) [optional conversion] │
│  │  │                                                                      │
│  │  ┌─ PERIODIC: step % H.steps_per_eval == 0 & step > 0                  │
│  │  │   ├─ FOR EACH val_batch:                                            │
│  │  │   │   valid_loss += sampler.train_iter(batch).item()                │
│  │  │   ├─ valid_loss /= num_batches                                      │
│  │  │   ├─ val_losses = np.append(val_losses, valid_loss)                 │
│  │  │   └─ log validation result                                          │
│  │  │                                                                      │
│  │  └─ PERIODIC: step % H.steps_per_checkpoint == 0 & step > load_step    │
│  │      ├─ save_model(sampler, H.sampler, step, H.log_dir) [log_utils]    │
│  │      │   └─ Save to: runs/{model_id}/checkpoints/model_{step}.th       │
│  │      │   └─ sync_to_home() → copy to home, keep only latest            │
│  │      ├─ save_model(optim, f'{H.sampler}_optim', step, ...)             │
│  │      ├─ save_model(ema_sampler, f'{H.sampler}_ema', ...) if H.ema      │
│  │      └─ save_stats(H, train_stats, step) [log_utils]                   │
│  │          └─ Save to: runs/{model_id}/stats/stats_{step}.pt             │
│  │          └─ sync_to_home() → copy to home                              │
│  └─ END FOR EACH STEP                                                      │
└────────────────────────────────────────────────────────────────────────────┘