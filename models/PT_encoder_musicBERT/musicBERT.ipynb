{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab18ed90",
   "metadata": {},
   "source": [
    "<!-- ![alt text](modelstructure.png \"model-structure\") -->\n",
    "<!-- ![alt text](modelstructure.png \"model-structure\") -->\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"modelstructure.png\" alt=\"model-structure\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef45cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.models.musicbert import MusicBERT, MusicBERTConfig\n",
    "from src.datasets.musicbert_dataset import MusicBERTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59c9475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 909\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "TARGET_BATCH_SIZE = 256 # The effective batch size we want to simulate\n",
    "BATCH_SIZE = 32 # Micro-batch size: Small enough to fit in GPU memory (Try 8 or 16)\n",
    "GRAD_ACCUM_STEPS = TARGET_BATCH_SIZE // BATCH_SIZE # Number of steps to accumulate\n",
    "\n",
    "MAX_SEQ_LEN = 1024\n",
    "# Optimized vocab sizes for OctupleMIDI (TimeSig, Tempo, Bar, Pos, Instr, Pitch, Dur, Vel)\n",
    "# +4 for special tokens (PAD, MASK, CLS, EOS)\n",
    "VOCAB_SIZES = [258, 53, 260, 132, 133, 132, 132, 36]\n",
    "\n",
    "# Use absolute path to ensure data is found on the server\n",
    "DATA_PATH = '../data/processed/'\n",
    "\n",
    "# Create Dataset\n",
    "# Ensure the data path exists and has .npy files\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"Warning: {DATA_PATH} does not exist. Please check the path.\")\n",
    "else:\n",
    "    dataset = MusicBERTDataset(DATA_PATH, max_seq_len=MAX_SEQ_LEN, vocab_sizes=VOCAB_SIZES)\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb6cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicBERT(\n",
      "  (element_embeddings): ModuleList(\n",
      "    (0): Embedding(258, 512)\n",
      "    (1): Embedding(53, 512)\n",
      "    (2): Embedding(260, 512)\n",
      "    (3): Embedding(132, 512)\n",
      "    (4): Embedding(133, 512)\n",
      "    (5-6): 2 x Embedding(132, 512)\n",
      "    (7): Embedding(36, 512)\n",
      "  )\n",
      "  (linear): Linear(in_features=4096, out_features=512, bias=True)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifiers): ModuleList(\n",
      "    (0): Linear(in_features=512, out_features=258, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=53, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=260, bias=True)\n",
      "    (3): Linear(in_features=512, out_features=132, bias=True)\n",
      "    (4): Linear(in_features=512, out_features=133, bias=True)\n",
      "    (5-6): 2 x Linear(in_features=512, out_features=132, bias=True)\n",
      "    (7): Linear(in_features=512, out_features=36, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Model Configuration\n",
    "config = MusicBERTConfig(\n",
    "    vocab_sizes=VOCAB_SIZES,\n",
    "    element_embedding_size=512,\n",
    "    hidden_size=512,\n",
    "    num_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    ffn_inner_hidden_size=2048,\n",
    "    dropout=0.1,\n",
    "    max_position_embeddings=MAX_SEQ_LEN,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "# Initialize Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MusicBERT(config).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb1968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaebd1a1",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-batch size: 32\n",
      "Gradient Accumulation steps: 8\n",
      "Effective Batch Size: 256\n",
      "Effective steps per epoch: 3\n",
      "Total epochs needed: 1250\n",
      "Starting Epoch 1/1250\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "TOTAL_STEPS = 1250*3\n",
    "WARMUP_STEPS = 250*3\n",
    "PEAK_LR = 5e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "BETAS = (0.9, 0.98)\n",
    "EPS = 1e-6\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1.0, betas=BETAS, eps=EPS, weight_decay=WEIGHT_DECAY) # lr set by scheduler\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "def lr_lambda(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        return (step + 1) / WARMUP_STEPS * PEAK_LR\n",
    "    else:\n",
    "        # Linear decay\n",
    "        decay_steps = TOTAL_STEPS - WARMUP_STEPS\n",
    "        current_decay_step = step - WARMUP_STEPS\n",
    "        return max(0.0, PEAK_LR * (1 - current_decay_step / decay_steps))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignore PAD token (0)\n",
    "\n",
    "# Ensure checkpoints directory exists\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "CHECKPOINT_PATH = '../checkpoints/musicbert_latest.pth'\n",
    "\n",
    "if 'train_loader' in locals():\n",
    "    # Calculate epochs\n",
    "    steps_per_epoch = len(train_loader) // GRAD_ACCUM_STEPS\n",
    "    \n",
    "    if steps_per_epoch > 0:\n",
    "        num_epochs = int(np.ceil(TOTAL_STEPS / steps_per_epoch))\n",
    "\n",
    "        print(f\"Micro-batch size: {BATCH_SIZE}\")\n",
    "        print(f\"Gradient Accumulation steps: {GRAD_ACCUM_STEPS}\")\n",
    "        print(f\"Effective Batch Size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "        print(f\"Effective steps per epoch: {steps_per_epoch}\")\n",
    "        print(f\"Total epochs needed: {num_epochs}\")\n",
    "\n",
    "        # Training Loop\n",
    "        model.train()\n",
    "        global_step = 0\n",
    "        optimizer.zero_grad() # Initialize gradients\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits_list = model(input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                loss = 0\n",
    "                # Calculate loss for each of the 8 attributes\n",
    "                for i in range(8):\n",
    "                    # Use specific vocab size for this attribute\n",
    "                    vocab_size = VOCAB_SIZES[i]\n",
    "                    output_flat = logits_list[i].view(-1, vocab_size)\n",
    "                    target_flat = labels[:, :, i].reshape(-1)\n",
    "                    loss += criterion(output_flat, target_flat)\n",
    "                \n",
    "                # Normalize loss for gradient accumulation\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "                loss.backward()\n",
    "                \n",
    "                # Add to epoch loss (multiply back to get actual loss value for logging)\n",
    "                epoch_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "                \n",
    "                # Step optimizer only after accumulating enough gradients\n",
    "                if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                \n",
    "                    # Print progress occasionally (based on updates)\n",
    "                    if global_step % 10 == 0:\n",
    "                         current_lr = scheduler.get_last_lr()[0]\n",
    "                         # loss.item() is scaled, so multiply by GRAD_ACCUM_STEPS for display\n",
    "                         print(f\"Epoch {epoch+1}, Step {global_step}, Loss: {loss.item() * GRAD_ACCUM_STEPS:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "                    if global_step >= TOTAL_STEPS:\n",
    "                        break\n",
    "            \n",
    "            # Calculate average loss over the number of micro-batches\n",
    "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "            \n",
    "            # Save model weights\n",
    "            torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
    "            print(f\"Model weights saved to {CHECKPOINT_PATH}\")\n",
    "            \n",
    "            if global_step >= TOTAL_STEPS:\n",
    "                print(\"Reached total training steps.\")\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        print(\"Train loader is empty.\")\n",
    "else:\n",
    "    print(\"Train loader not defined.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
