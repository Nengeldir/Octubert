{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b11a51",
   "metadata": {},
   "source": [
    "# Data Exploration and Setup\n",
    "This notebook demonstrates how to use the project's source code for data loading and exploration.\n",
    "We are using a hybrid approach: core logic is in `src/` and exploration is in `notebooks/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5d745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../checkpoints/musicbert_latest.pth\n",
      "Model loaded and set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from ...models.musicbert import MusicBERT, MusicBERTConfig\n",
    "from ...data.musicbert import MusicBERTDataset\n",
    "\n",
    "# Configuration (Must match training config)\n",
    "MAX_SEQ_LEN = 1024\n",
    "# Optimized vocab sizes for OctupleMIDI (TimeSig, Tempo, Bar, Pos, Instr, Pitch, Dur, Vel)\n",
    "# +4 for special tokens (PAD, MASK, CLS, EOS)\n",
    "VOCAB_SIZES = [258, 53, 260, 132, 133, 132, 132, 36]\n",
    "\n",
    "config = MusicBERTConfig(\n",
    "    vocab_sizes=VOCAB_SIZES,\n",
    "    element_embedding_size=512,\n",
    "    hidden_size=512,\n",
    "    num_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    ffn_inner_hidden_size=2048,\n",
    "    dropout=0.1,\n",
    "    max_position_embeddings=MAX_SEQ_LEN,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "# Initialize Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MusicBERT(config).to(device)\n",
    "\n",
    "# Load Checkpoint\n",
    "CHECKPOINT_PATH = '../checkpoints/musicbert_latest.pth'\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Loading checkpoint from {CHECKPOINT_PATH}\")\n",
    "    state_dict = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    print(\"Model loaded and set to evaluation mode.\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0938329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with input shape: torch.Size([1, 128, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lziltener/.local/lib/python3.13/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful!\n",
      "Number of output heads: 8\n",
      "Output shape per head: torch.Size([1, 128, 258])\n"
     ]
    }
   ],
   "source": [
    "# Test with dummy input\n",
    "batch_size = 1\n",
    "seq_len = 128 # Arbitrary sequence length for testing\n",
    "\n",
    "# Generate random input (batch_size, seq_len, 8)\n",
    "# We need to respect vocab sizes for each attribute\n",
    "dummy_input = torch.zeros((batch_size, seq_len, 8), dtype=torch.long).to(device)\n",
    "for i in range(8):\n",
    "    dummy_input[:, :, i] = torch.randint(0, VOCAB_SIZES[i], (batch_size, seq_len)).to(device)\n",
    "\n",
    "# Generate dummy attention mask (batch_size, seq_len)\n",
    "# False indicates valid tokens, True indicates padding (ignored)\n",
    "dummy_mask = torch.zeros((batch_size, seq_len), dtype=torch.bool).to(device)\n",
    "\n",
    "print(f\"Testing model with input shape: {dummy_input.shape}\")\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        logits_list = model(dummy_input, attention_mask=dummy_mask)\n",
    "    \n",
    "    print(\"Forward pass successful!\")\n",
    "    print(f\"Number of output heads: {len(logits_list)}\")\n",
    "    print(f\"Output shape per head: {logits_list[0].shape}\") # Should be (batch_size, seq_len, vocab_size)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during forward pass: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b0b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 909\n",
      "Input shape: torch.Size([1, 1024, 8])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results (First 20 tokens) ---\n",
      "Index  | Original                       | Masked Input                   | Predicted                     \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "0      | [2 2 2 2 2 2 2 2]              |   [2 2 2 2 2 2 2 2]            | [2 2 2 2 2 2 2 2]             \n",
      "1      | [ 5 29  4 70 11 34 11 34]      |   [ 5 29  4 70 11 34 11 34]    | [ 5 29  4 70 11 34 11 34]     \n",
      "2      | [ 6  5  4 51 23 20 11 34]      | * [ 6  5  4  1 23  1 11 34]    | [ 6  5  4 48 23 27 11 34]     \n",
      "3      | [ 6  5  4 79 12 34 11 34]      | * [ 6  5  4  1 12  1 11 34]    | [ 6  5  4 67 12 27 11 34]     \n",
      "4      | [ 6  9  4 58 21 16 11 34]      | * [ 6  9  4  1 21  1 11 34]    | [ 6  9  4 67 21 27 11 34]     \n",
      "5      | [ 6 13  4 63 17 18 11 34]      | * [ 6 13  4  1 17  1 11 34]    | [ 6 13  4 67 17 29 11 34]     \n",
      "6      | [ 6 13  4 77  9 33 11 34]      | * [ 6 13  4  1  9  1 11 34]    | [ 6 13  4 60  9 29 11 34]     \n",
      "7      | [ 6 17  4 70 22 29 11 34]      | * [ 6 17  4  1 22  1 11 34]    | [ 6 17  4 65 22 29 11 34]     \n",
      "8      | [ 6 21  4 75  9 34 11 34]      | * [ 6 21  4  1  9  1 11 34]    | [ 6 21  4 62  9 31 11 34]     \n",
      "9      | [ 6 29  4 84  7 33 11 34]      | * [ 6 29  4  1  7  1 11 34]    | [ 6 29  4 67  7 30 11 34]     \n",
      "10     | [ 6 33  4 86  8 30 11 34]      | * [ 6 33  4  1  8  1 11 34]    | [ 6 33  4 60  8 29 11 34]     \n",
      "11     | [ 7  5  4 53 22 28 11 34]      |   [ 7  5  4 53 22 28 11 34]    | [ 7  5  4 53 22 28 11 34]     \n",
      "12     | [ 7  5  4 84 22 34 11 34]      |   [ 7  5  4 84 22 34 11 34]    | [ 7  5  4 84 22 34 11 34]     \n",
      "13     | [ 7  9  4 60 19 21 11 34]      |   [ 7  9  4 60 19 21 11 34]    | [ 7  9  4 60 19 21 11 34]     \n",
      "14     | [ 7 13  4 65 19 25 11 34]      |   [ 7 13  4 65 19 25 11 34]    | [ 7 13  4 65 19 25 11 34]     \n",
      "15     | [ 7 17  4 69 15 24 11 34]      |   [ 7 17  4 69 15 24 11 34]    | [ 7 17  4 69 15 24 11 34]     \n",
      "16     | [ 7 29  4 70 10 33 11 34]      |   [ 7 29  4 70 10 33 11 34]    | [ 7 29  4 70 10 33 11 34]     \n",
      "17     | [ 8  5  4 50 25 25 11 34]      | * [ 8  5  4 50 25 25  1 34]    | [ 8  5  4 50 25 25 11 34]     \n",
      "18     | [ 8  5  4 79 13 33 11 34]      | * [ 8  5  4 79 13 33  1 34]    | [ 8  5  4 79 13 33 11 34]     \n",
      "19     | [ 8  9  4 57 23 22 11 34]      | * [ 8  9  4 57 23 22  1 34]    | [ 8  9  4 57 23 22 11 34]     \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use the Dataset class to handle loading and masking\n",
    "DATA_PATH = '../data/processed/'\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    # Initialize Dataset\n",
    "    dataset = MusicBERTDataset(DATA_PATH, max_seq_len=MAX_SEQ_LEN, vocab_sizes=VOCAB_SIZES)\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    \n",
    "    if len(dataset) > 0:\n",
    "        # Get a sample (e.g., index 0)\n",
    "        sample_idx = 0\n",
    "        sample = dataset[sample_idx]\n",
    "        \n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device) # (1, seq_len, 8)\n",
    "        labels = sample['labels'].unsqueeze(0).to(device)       # (1, seq_len, 8)\n",
    "        attention_mask = sample['attention_mask'].unsqueeze(0).to(device) # (1, seq_len)\n",
    "        \n",
    "        print(f\"Input shape: {input_ids.shape}\")\n",
    "        \n",
    "        # Run Model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_list = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "        # Get Predictions\n",
    "        # logits_list is list of 8 tensors, each (batch, seq_len, vocab_size)\n",
    "        predictions = []\n",
    "        for i in range(8):\n",
    "            # Get max over vocab dimension\n",
    "            pred_tokens = torch.argmax(logits_list[i], dim=-1) # (batch, seq_len)\n",
    "            predictions.append(pred_tokens)\n",
    "            \n",
    "        # Stack predictions to get (batch, seq_len, 8)\n",
    "        predicted_tokens = torch.stack(predictions, dim=2).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Convert inputs and labels to numpy for display\n",
    "        input_ids_np = input_ids.squeeze(0).cpu().numpy()\n",
    "        labels_np = labels.squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Print Results\n",
    "        print(\"\\n--- Results (First 20 tokens) ---\")\n",
    "        print(f\"{'Index':<6} | {'Original':<30} | {'Masked Input':<30} | {'Predicted':<30}\")\n",
    "        print(\"-\" * 105)\n",
    "        \n",
    "        for i in range(min(20, len(input_ids_np))):\n",
    "            orig = str(labels_np[i])\n",
    "            masked = str(input_ids_np[i])\n",
    "            pred = str(predicted_tokens[i])\n",
    "            \n",
    "            # Highlight if masked (input != label)\n",
    "            is_masked = not np.array_equal(labels_np[i], input_ids_np[i])\n",
    "            marker = \"*\" if is_masked else \" \"\n",
    "            \n",
    "            print(f\"{i:<6} | {orig:<30} | {marker} {masked:<28} | {pred:<30}\")\n",
    "    else:\n",
    "        print(\"Dataset is empty.\")\n",
    "else:\n",
    "    print(f\"Data path {DATA_PATH} not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
